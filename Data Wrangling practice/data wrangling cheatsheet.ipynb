{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data wrangling is the process of gathering your data, assessing its quality and structure, and cleaning it before you do things like analysis, visualisation, or build predictive models using machine learning.\n",
    "\n",
    "![](images/data_wrangling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it's as simple as downloading a file, spotting a few typos, and fixing those typos. But other times, your data really isn't clean i.e. you'll have missing records, duplicates and inaccurate data. Sometimes the data itself is fine but structurally, it's difficult to work with. Taking care of all this is necessary or else, you risk making mistakes, missing insights and wasting time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangling means to round up, herd, or take charge of livestock, like horses or sheep. Let's focus in on the sheep example.\n",
    "\n",
    "A shepherd's main goals are to get their sheep to their pastures to let them graze, guide them to market to shear them, and put them in the barn to sleep. Before any of that though, they must be rounded up in a nice and organized group. The consequences if they're not? These tasks take longer. If they're all scattered, some could also run off and get lost. A wolf could even sneak into the pack and feast on a few of them.\n",
    "\n",
    "An Analogy:\n",
    "<br>\n",
    "The same idea of organizing before acting is true for those who are shepherds of data. We need to wrangle our data for good outcomes, otherwise there could be consequences. If we analyze, visualize, or model our data before we wrangle it, our consequences could be making mistakes, missing out on cool insights, and wasting time. So best practices say wrangle. Always.\n",
    "\n",
    "The development of Python and its libraries have made wrangling easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather (Intro)\n",
    "Gathering data is always the first step in data wrangling. The idea is before gathering, we have no data and after it, we do. This is sometimes called acquiring your data, or collecting it. A bit of scurrying is often required plus some unexpectedness. And depending upon where you find your data and what format it's in, the steps of the gathering process can vary. If the data is in a file, gathering often means downloading the file and importing it in your programming environment like a Jupyter notebook. Other methods of gathering are things like collecting data from files and databases which is what you'll usually do in the workplace. Or you can scrape data off a website or get it from an API, which stands for application programming interface. API's let us programmatically access data from applications like Twitter and Facebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess (Intro)\n",
    "After gathering data, we need to assess our data to determine what's clean and potentially what else to gather if we're missing some data. We're not exploring our dataset. We just want to make sure our data is an a form that makes our analysis easier later on. Okay. So what are we assessing? What is dirty data? What is messy data? We're looking for two main things: our data's quality and its tidiness. Low quality data is dirty. Untidy data is messy.\n",
    "\n",
    "#### Quality\n",
    "Low quality data is commonly referred to as dirty data. Dirty data has issues with its content.\n",
    "\n",
    "Imagine you had a table with two columns: Name and Height, like below:\n",
    "\n",
    "![](images/table1.png)\n",
    "\n",
    "Common data quality issues include:\n",
    "* missing data, like the missing height value for Juan.\n",
    "* invalid data, like a cell having an impossible value, e.g., like negative height value for Kwasi. Having \"inches\" and \"centimetres\" in the height entries is technically invalid as well, since the datatype for height becomes a string when those are present. The datatype for height should be integer or float.\n",
    "* inaccurate data, like Jane actually being 58 inches tall, not 55 inches tall.\n",
    "* inconsistent data, like using different units for height (inches and centimetres).\n",
    "\n",
    "Data quality is a perception or an assessment of data's fitness to serve its purpose in a given context. Unfortunately, that’s a bit of an evasive definition but it gets to something important: there are no hard and fast rules for data quality. One dataset may be high enough quality for one application but not for another.\n",
    "\n",
    "#### Tidiness\n",
    "_Untidy data_ is commonly referred to as _\"messy\" data_. Messy data has issues with its structure. Tidy data is a relatively new concept coined by statistician, professor, and all-round data expert Hadley Wickham. I’m going to take a quote from his excellent paper on the subject:\n",
    "\n",
    "It is often said that 80% of data analysis is spent on the cleaning and preparing data. And it’s not just a first step, but it must be repeated many times over the course of analysis as new problems come to light or new data is collected. To get a handle on the problem, this paper focuses on a small, but important, aspect of data cleaning that I call data tidying: structuring datasets to facilitate analysis.\n",
    "\n",
    "A dataset is messy or tidy depending on how rows, columns, and tables are matched up with observations, variables, and types. In tidy data:\n",
    "* Each variable forms a column.\n",
    "* Each observation forms a row.\n",
    "* Each type of observational unit forms a table.\n",
    "\n",
    "![](images/tidy_data.png)\n",
    "\n",
    "#### Types of Assessment\n",
    "* Visual Assessment - Visual assessment is simple. Open your data in your favorite software application (Google Sheets, Excel, a text editor, etc.) and scroll through it, looking for quality and tidiness issues.\n",
    "* Programmatic Assessment - Programmatic assessment tends to be more efficient than visual assessment. One simple example of a programmatic assessment is pandas' info method, which gives us the basic info of your DataFrame—like number of entries, number of columns, the types of each column, whether there are missing values, and more. Some other pandas' methods used for visual assessment are namely:\n",
    "    - head\n",
    "    - tail\n",
    "    - shape\n",
    "    - value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean (Intro)\n",
    "Now we've gathered the data and made a few assessments. The few is important. We don't need to identify every issue right from the start. We can iterate. So now we can start cleaning. Cleaning means acting on the assessments we made to improve quality and tidiness.\n",
    "\n",
    "#### Improving Quality\n",
    "Improving quality doesn't mean changing the data to make it say something different. That would be data fraud. Instead, we're talking about things like correcting it when it's inaccurate or removing data when it's wrong or irrelevant, or replacing, like filling in missing values, or merging, like combining gathered datasets that was split up.<br>\n",
    "Consider the animals DataFrame, which has headers for name, body weight (in kilograms), and brain weight (in grams). The last five rows of this DataFrame are displayed below:\n",
    "\n",
    "![](images/animals_df.png)\n",
    "\n",
    "Examples of improving quality include:\n",
    "* Correcting when inaccurate, like correcting the mouse's body weight to 0.023 kg instead of 230 kg.\n",
    "* Removing when irrelevant, like removing the row with \"Apple\" since an apple is a fruit and not an animal.\n",
    "* Replacing when missing, like filling in the missing value for brain weight for Brachiosaurus.\n",
    "* Combining, like concatenating the missing rows in the more_animals DataFrame displayed below\n",
    "\n",
    "![](images/more_animals_df.png)\n",
    "\n",
    "All of this stuff can be done manually, but it's most efficiently done using code that minimises repetition.\n",
    "\n",
    "#### Improving Tidiness\n",
    "Improving tidiness means transforming the dataset so that each variable is a column, each observation is a row, and each type of observational unit is a table. There are special functions in pandas that help us do that. We'll dive deeper into those in this notebook ahead.\n",
    "\n",
    "#### Programmatic Data Cleaning Process\n",
    "The programmatic data cleaning process:\n",
    "1. Define\n",
    "2. Code\n",
    "3. Test\n",
    "\n",
    "**Defining** means defining a data cleaning plan in writing, where we turn our assessments into defined cleaning tasks. This plan will also serve as an instruction list so others (or us in the future) can look at our work and reproduce it.\n",
    "\n",
    "**Coding** means translating these definitions to code and executing that code.\n",
    "\n",
    "**Testing** means testing our dataset, often using code, to make sure our cleaning operations worked.\n",
    "\n",
    "We've gathered, assessed and just cleaned our data. Are we done? No. After cleaning, we always reassess and then iterate on any the steps if we need to. If we're happy for with the quality and tidiness of our data, we can end our wrangling process and move on to storing our clean data, or analysing, visualising, or modeling it. Sometimes we realise we need to gather more data. Sometimes we miss assessments. It's hard to catch everything on the first go, at it's also very common to find new issues as you're fixing the ones you've already identified. Sometimes our cleaning operations don't work as we intended. once we go through each step once, we can revisit any step in the process any time, even after we've finished wrangling and moved on to analysis, visualisation, or modeling.\n",
    "\n",
    "![](images/reasses_iterate.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
